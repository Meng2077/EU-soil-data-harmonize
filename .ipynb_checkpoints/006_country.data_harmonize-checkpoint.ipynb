{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1675282d-9508-4525-b0b7-a1867a448dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocess as mp\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/opt/conda/share/proj')\n",
    "import pandas as pd\n",
    "# import dbfread \n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pyproj\n",
    "# from simpledbf import Dbf5\n",
    "from datetime import datetime\n",
    "\n",
    "input_path = '/mnt/inca/ai4sh_data.harmo/raw_data'\n",
    "output_path = '/mnt/inca/ai4sh_data.harmo/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861efafe-fbee-40cc-818b-12c167dd2666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6271 data in total\n",
      "2556 data with no time info\n",
      "339 data with no depth info\n",
      "0 data with no coordinate info\n"
     ]
    }
   ],
   "source": [
    "# crotia, harmonized dataset from MultiOne\n",
    "crotia = pd.read_excel(f'{input_path}/crotia_multione/hr_topsoil_db.xlsx')\n",
    "\n",
    "# organize the depth\n",
    "crotia.loc[crotia['dbr'].isna(), 'hzn_top'] = np.nan\n",
    "crotia.loc[crotia['dbr'].isna(), 'hzn_btm'] = np.nan\n",
    "crotia.loc[~crotia['dbr'].isna(), 'hzn_top'] = crotia.loc[~crotia['dbr'].isna(), 'dbr'] - 10\n",
    "crotia.loc[~crotia['dbr'].isna(), 'hzn_btm'] = crotia.loc[~crotia['dbr'].isna(), 'dbr'] + 10\n",
    "crotia.loc[crotia['source_db'].isin(['agricultural_2013','azo_2016']), 'hzn_btm'] = 30\n",
    "crotia.loc[crotia['source_db'].isin(['agricultural_2013','azo_2016']), 'hzn_top'] = 0\n",
    "crotia.loc[crotia['source_db'] == 'azo_2013', 'hzn_top'] = 0\n",
    "crotia.loc[crotia['source_db'] == 'azo_2013', 'hzn_btm'] = 25\n",
    "\n",
    "column_names = ['lat','lon','time','hzn_top','hzn_btm','ref']\n",
    "temp = pd.DataFrame(columns=column_names)\n",
    "temp['lat'] = crotia['latitude_decimal_degrees']\n",
    "temp['lon'] = crotia['longitude_decimal_degrees']\n",
    "temp['nuts0'] = 'HR'\n",
    "temp['time'] = crotia['site_obsdate']\n",
    "temp['hzn_top'] = crotia['hzn_top']\n",
    "temp['hzn_btm'] = crotia['hzn_btm']\n",
    "temp['ref'] = 'croatia.multione-'+ crotia['source_db']\n",
    "temp['oc'] = crotia['oc']*10 # % -> g/kg\n",
    "temp['ph_cacl2'] = (crotia['ph_kcl']+0.09)*0.987+0.321 # convert from ph_kcl to ph_cacl2\n",
    "temp['ph_h2o'] = crotia['ph_h2o']\n",
    "# temp['ph_cacl2'] = np.nan\n",
    "temp['bulk_density'] = crotia['db_od']\n",
    "temp['clay'] = crotia['clay_tot_psa']\n",
    "temp['silt'] = crotia['silt_tot_psa']\n",
    "temp['sand'] = crotia['sand_tot_psa']\n",
    "temp['caco3'] = crotia['caco3']*10 # % -> g/kg\n",
    "temp['N'] = crotia['n_tot_ncs']*10 # % -> g/kg\n",
    "temp['K'] = crotia['k_mehlich3']*0.965 + 7.13 # mehlich convert to AAE\n",
    "# temp['CEC'] = crotia['cec_sum']\n",
    "temp['EC'] = crotia['ec_satp']*100 # dS/m ->  mS/m\n",
    "# temp['P'] = crotia['p_mehlich3'] # mehlich3 - olsen method  not convertable\n",
    "\n",
    "# basic info\n",
    "print(f'{len(temp)} data in total')\n",
    "\n",
    "na = temp['time'].isna().sum()\n",
    "print(f'{na} data with no time info')\n",
    "\n",
    "na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\n",
    "print(f'{na} data with no depth info')\n",
    "\n",
    "na = len(temp[temp['lat'].isna() | temp['lon'].isna()])\n",
    "print(f'{na} data with no coordinate info')\n",
    "\n",
    "temp.to_csv(f'{output_path}/croatia_harmonized_v1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d9baba-b03e-4e2c-b742-d0f211e61f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17189 data in total\n",
      "0 data with no time info\n",
      "0 data with no depth info\n",
      "0 data with no coordinate info\n"
     ]
    }
   ],
   "source": [
    "# germany\n",
    "germany = pd.read_excel(f'{input_path}/Germany/LABORATORY_DATA.xlsx', engine='openpyxl')\n",
    "germany_site = pd.read_excel(f'{input_path}/Germany/SITE.xlsx', engine='openpyxl')\n",
    "germany = germany.merge(germany_site, on=\"PointID\", how=\"inner\")\n",
    "utm_projection = pyproj.CRS.from_string(f'+proj=utm +zone={32} +ellps=WGS84')\n",
    "gps_projection = pyproj.CRS.from_epsg(4326)\n",
    "# Create transformer objects for the coordinate conversion\n",
    "transformer = pyproj.Transformer.from_crs(utm_projection, gps_projection)\n",
    "# Convert UTM coordinates to GPS latitude and longitude\n",
    "germany['lat'], germany['lon'] = transformer.transform(germany['xcoord'], germany['ycoord'])\n",
    "\n",
    "column_names = ['lat','lon','time','hzn_top','hzn_btm','ref']\n",
    "temp = pd.DataFrame(columns=column_names)\n",
    "temp['time'] = germany['Sampling_year']\n",
    "temp['hzn_top'] = germany['Layer upper limit']\n",
    "temp['hzn_btm'] = germany['Layer lower limit']\n",
    "temp['ref'] = 'germany.thuenen-'+germany['County_x']\n",
    "temp['lat'] = germany['lat']\n",
    "temp['lon'] = germany['lon']\n",
    "temp['nuts0'] = 'DE'\n",
    "temp['oc'] = germany['TOC']\n",
    "temp['N'] = germany['TN']\n",
    "temp['ph_kcl'] = np.nan\n",
    "temp['ph_h2o'] = germany['pH_H2O']\n",
    "temp['ph_cacl2'] = germany['pH_CaCl2']\n",
    "temp['bulk_density'] = germany['BD_bulk']\n",
    "temp['clay'] = germany['Clay']\n",
    "temp['silt'] = germany['Silt']\n",
    "temp['sand'] = germany['Sand']\n",
    "temp['caco3'] = np.nan\n",
    "temp['K'] = np.nan\n",
    "temp['P'] = np.nan\n",
    "temp['EC'] = germany['EC_H2O']*0.0001 # µS/cm -> dS/m\n",
    "\n",
    "# basic info\n",
    "print(f'{len(temp)} data in total')\n",
    "\n",
    "na = temp['time'].isna().sum()\n",
    "print(f'{na} data with no time info')\n",
    "\n",
    "na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\n",
    "print(f'{na} data with no depth info')\n",
    "\n",
    "na = len(temp[temp['lat'].isna() | temp['lon'].isna()])\n",
    "print(f'{na} data with no coordinate info')\n",
    "\n",
    "temp.to_csv(f'{output_path}/germany_harmonized_v1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5fb6f4e-a80f-49f3-a869-907d3126c7ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "DBFNotFound",
     "evalue": "could not find file '/mnt/inca/ai4sh_data.harmo/raw_data/Slovenia/Horizonti_v_PedoloskihProfilih.DBF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDBFNotFound\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1220/260913242.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{input_path}/Slovenia/Horizonti_v_PedoloskihProfilih.DBF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'K'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/dbfread/dbf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, encoding, ignorecase, lowernames, parserclass, recfactory, load, raw, ignore_missing_memofile, char_decode_errors)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDBFNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'could not find file {!r}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDBFNotFound\u001b[0m: could not find file '/mnt/inca/ai4sh_data.harmo/raw_data/Slovenia/Horizonti_v_PedoloskihProfilih.DBF'"
     ]
    }
   ],
   "source": [
    "# slovenia\n",
    "from dbfread import DBF\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "dbf = DBF(f'{input_path}/Slovenia/Horizonti_v_PedoloskihProfilih.DBF', encoding='latin-1')\n",
    "df = pd.DataFrame(iter(dbf))\n",
    "df = df.drop(columns=['K'])\n",
    "gdf = gpd.read_file(f'{input_path}/Slovenia/PedoloskiProfili.shp')\n",
    "gdf = gdf.to_crs('epsg:4326')\n",
    "gdf['lon'] = gdf['geometry'].x\n",
    "gdf['lat'] = gdf['geometry'].y\n",
    "\n",
    "slovenia = pd.merge(gdf, df, on='ZPP')\n",
    "column_name_mapping = {\n",
    "    'LETO': 'time',\n",
    "    'GLZG': 'hzn_top',\n",
    "    'GLSP': 'hzn_btm',\n",
    "    'OS': 'oc',\n",
    "    'DUSIK' : 'N',\n",
    "    'GLINA': 'clay',\n",
    "    'MELJ': 'silt',\n",
    "    'PES': 'sand',\n",
    "    'KALIJ': 'K',\n",
    "    'FOSF': 'P',\n",
    "    'PHH':'ph_h2o',\n",
    "    'PHCA':'ph_cacl2',\n",
    "    'T': 'CEC'\n",
    "    # 'PHK':'ph_kcl'\n",
    "}\n",
    "\n",
    "slovenia = slovenia.rename(columns=column_name_mapping)\n",
    "\n",
    "slovenia.loc[slovenia['ph_cacl2']==0,'ph_cacl2'] = np.nan\n",
    "slovenia.loc[slovenia['ph_h2o']==0,'ph_h2o'] = np.nan\n",
    "slovenia['oc'] = slovenia['oc']*10*1.3 # % -> g/kg\n",
    "slovenia['N'] = slovenia['N']*10 # % -> g/kg\n",
    "slovenia['K'] = slovenia['K']*0.9/10 # mg/100g -> mg/kg\n",
    "slovenia['P'] = np.nan # drop due to low correlation, slovenia['P']/10 # mg/100g -> mg/kg\n",
    "slovenia = slovenia[['lat','lon','P', 'K', 'oc', 'N','sand','silt','clay','time','hzn_top','hzn_btm','ph_h2o','ph_cacl2','CEC']]\n",
    "\n",
    "temp = slovenia\n",
    "# possible filter\n",
    "naa = len(temp.loc[temp['time'] == 0])\n",
    "temp = temp.loc[temp['time'] != 0]\n",
    "na = temp['time'].isna().sum()\n",
    "print(f'{na+naa} data with no time info')\n",
    "\n",
    "na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\n",
    "print(f'{na} data with no depth info')\n",
    "\n",
    "na = len(temp[temp['lat'].isna() | temp['lon'].isna()])\n",
    "print(f'{na} data with no coordinate info')\n",
    "\n",
    "print(f'{len(temp)} in total')\n",
    "temp['ref'] = 'slovenia-Pedološka'\n",
    "temp['nuts0'] = 'SI'\n",
    "temp.to_csv(f'{output_path}/slovenia_harmonized_v1.csv',index=False)\n",
    "# slovenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da958f00-e769-40f4-800e-02155ed9fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data with no time info\n",
      "0 data with no depth info\n",
      "0 data with no coordinate info\n",
      "1343 in total\n"
     ]
    }
   ],
   "source": [
    "# estonia\n",
    "temp = pd.read_csv(f'{input_path}/estonia_harmonized_v1.csv')\n",
    "temp['ref'] = 'estonia.kese'\n",
    "temp['nuts0'] = 'EE'\n",
    "\n",
    "# possible filter\n",
    "na = temp['time'].isna().sum()\n",
    "print(f'{na} data with no time info')\n",
    "\n",
    "na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\n",
    "print(f'{na} data with no depth info')\n",
    "\n",
    "na = len(temp[temp['lat'].isna() | temp['lon'].isna()])\n",
    "print(f'{na} data with no coordinate info')\n",
    "\n",
    "print(f'{len(temp)} in total')\n",
    "temp.to_csv(f'{output_path}/estonia_harmonized_v1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5283b8fb-2a0d-4d32-b589-5470e952d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data with no time info\n",
      "0 data with no depth info\n",
      "0 data with no coordinate info\n",
      "1070 in total\n"
     ]
    }
   ],
   "source": [
    "# netherlands\n",
    "temp = gpd.read_file(f'{input_path}/netherlands/nl.4326.gpkg')\n",
    "temp['ref'] = 'netherland.BHR-P'\n",
    "temp['nuts0'] = 'NL'\n",
    "\n",
    "name_mapping = {'begin_depth':'hzn_top',\n",
    "                'end_depth':'hzn_btm',\n",
    "               'dry_bulk_density':'bulk_density'}\n",
    "temp['time'] = temp['research_report_date'].str[0:4].astype(int)\n",
    "temp = temp.rename(columns=name_mapping)\n",
    "temp['lat'] = temp['geometry'].y\n",
    "temp['lon'] = temp['geometry'].x\n",
    "temp = temp.drop(columns = ['organic_matter_content','research_report_date','geometry'])\n",
    "temp['hzn_top'] = temp['hzn_top']*100\n",
    "temp['hzn_btm'] = temp['hzn_btm']*100\n",
    "\n",
    "# possible filter\n",
    "na = temp['time'].isna().sum()\n",
    "print(f'{na} data with no time info')\n",
    "\n",
    "na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\n",
    "print(f'{na} data with no depth info')\n",
    "\n",
    "na = len(temp[temp['lat'].isna() | temp['lon'].isna()])\n",
    "print(f'{na} data with no coordinate info')\n",
    "\n",
    "print(f'{len(temp)} in total')\n",
    "temp.to_csv(f'{output_path}/netherlands_harmonized_v1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949eeee7-a316-40a0-980e-71bdf4d255b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "/mnt/inca/ai4sh_data.harmo/raw_data/GEMAS/GEMAS.csv: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: /mnt/inca/ai4sh_data.harmo/raw_data/GEMAS/GEMAS.csv: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1220/1963377928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# gema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{input_path}/GEMAS/GEMAS.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hzn_top'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hzn_btm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fiona\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         return _read_file_fiona(\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             colxn = Collection(\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: /mnt/inca/ai4sh_data.harmo/raw_data/GEMAS/GEMAS.csv: No such file or directory"
     ]
    }
   ],
   "source": [
    "# gema\n",
    "gema = gpd.read_file(f'{input_path}/GEMAS/GEMAS.csv')\n",
    "\n",
    "column_names = ['lat','lon','time','hzn_top','hzn_btm','ref']\n",
    "temp = pd.DataFrame(columns=column_names)\n",
    "temp['lat'] = gema['YCOO']\n",
    "temp['lon'] = gema['XCOO']\n",
    "temp['oc'] = gema['TOC'] \n",
    "temp['ph_cacl2'] = gema['pH_CaCl2']\n",
    "temp['clay'] = gema['clay']\n",
    "temp['silt'] = gema['silt']\n",
    "temp['time'] = 2008\n",
    "temp['hzn_top'] = gema['UHDICM']\n",
    "temp['hzn_btm'] = gema['LHDICM']\n",
    "temp = temp.apply(pd.to_numeric)\n",
    "temp['sand'] = 100-temp['clay']-temp['silt']\n",
    "temp['oc'] = temp['oc']*10 # % -> g/kg\n",
    "temp['CEC'] = gema['CEC'] # meq/100g = cmol/kg\n",
    "\n",
    "country_to_nuts0 = {\n",
    "    'GER': 'DE',  # Germany\n",
    "    'SKA': 'SK',  # Slovakia\n",
    "    'EST': 'EE',  # Estonia\n",
    "    'LIT': 'LT',  # Lithuania\n",
    "    'NOR': 'NO',  # Norway (Note: Norway is not an EU member but is included in some NUTS classifications)\n",
    "    'PTG': 'PT',  # Portugal\n",
    "    'POL': 'PL',  # Poland\n",
    "    'SWE': 'SE',  # Sweden\n",
    "    'DEN': 'DK',  # Denmark\n",
    "    'ITA': 'IT',  # Italy\n",
    "    'FRA': 'FR',  # France\n",
    "    'FIN': 'FI',  # Finland\n",
    "    'UKR': 'UA',  # Ukraine (Note: Ukraine is not an EU member and typically not included in NUTS)\n",
    "    'CRO': 'HR',  # Croatia\n",
    "    'HEL': 'EL',  # Greece (Note: The code for Greece in the NUTS classification is EL, not GR)\n",
    "    'HUN': 'HU',  # Hungary\n",
    "    'SPA': 'ES',  # Spain\n",
    "    'CYP': 'CY',  # Cyprus\n",
    "    'BEL': 'BE',  # Belgium\n",
    "    'UNK': 'UK',  # United Kingdom (Note: The UK left the EU but was previously included in NUTS)\n",
    "    'LAV': 'LV',  # Latvia\n",
    "    'SIL': 'SI',  # Slovenia\n",
    "    'BUL': 'BG',  # Bulgaria\n",
    "    'SRB': 'RS',  # Serbia (Note: Serbia is a candidate country for EU membership)\n",
    "    'CZR': 'CZ',  # Czech Republic\n",
    "    'BOS': 'BA',  # Bosnia and Herzegovina (Note: Bosnia and Herzegovina is not an EU member)\n",
    "    'FOM': 'MK',  # North Macedonia (Note: The official NUTS code for North Macedonia is MK)\n",
    "    'AUS': 'AT',  # Austria\n",
    "    'NEL': 'NL',  # Netherlands\n",
    "    'SLO': 'SK',  # Slovakia (Note: This seems to be a duplicate of SKA)\n",
    "    'IRL': 'IE',  # Ireland\n",
    "    'MON': 'ME',  # Montenegro (Note: Montenegro is a candidate country for EU membership)\n",
    "    'LUX': 'LU'   # Luxembourg\n",
    "}\n",
    "\n",
    "temp['nuts0'] = gema['COUNTRY']\n",
    "temp['nuts0'] = temp['nuts0'].map(country_to_nuts0)\n",
    "temp['ref'] = 'gemas'\n",
    "temp['lc_survey'] = gema['TYPE']\n",
    "temp.loc[temp['lc_survey']=='Gr','lc_survey'] = 'permanent grassland'\n",
    "temp.loc[temp['lc_survey']=='Ap','lc_survey'] = 'arable land'\n",
    "\n",
    "# possible filter\n",
    "na = temp['time'].isna().sum()\n",
    "print(f'{na} data with no time info')\n",
    "\n",
    "na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\n",
    "print(f'{na} data with no depth info')\n",
    "\n",
    "na = len(temp[temp['lat'].isna() | temp['lon'].isna()])\n",
    "print(f'{na} data with no coordinate info')\n",
    "\n",
    "print(f'{len(temp)} in total')\n",
    "temp.to_csv(f'{output_path}/gemas_harmonized_v1.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81fb14af-e26c-45c0-8dd8-581aff68b3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data with no time info\n",
      "0 data with no depth info\n",
      "0 data with no coordinate info\n",
      "36036 in total\n"
     ]
    }
   ],
   "source": [
    "# swiss\n",
    "temp = pd.read_csv(f'{input_path}/NatbodDS_V6_EN/swiss_harmonized_v0.csv')\n",
    "temp['ref'] = 'swiss.nabo'\n",
    "temp['nuts0'] = 'CH'\n",
    "\n",
    "# possible filter\n",
    "na = temp['time'].isna().sum()\n",
    "print(f'{na} data with no time info')\n",
    "\n",
    "na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\n",
    "print(f'{na} data with no depth info')\n",
    "\n",
    "na = len(temp[temp['lat'].isna() | temp['lon'].isna()])\n",
    "print(f'{na} data with no coordinate info')\n",
    "\n",
    "print(f'{len(temp)} in total')\n",
    "\n",
    "temp = temp.drop(columns=['anonymization','date','ID'])\n",
    "temp.to_csv(f'{output_path}/swiss_harmonized_v1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e34ee9c-b805-4975-adf8-6a01049aaa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data with no time info\n",
      "0 data with no depth info\n",
      "0 data with no coordinate info\n",
      "376 in total\n"
     ]
    }
   ],
   "source": [
    "# foregs\n",
    "temp = pd.read_csv(f'{input_path}/foregs/foregs_harmonized_v0.csv')\n",
    "temp['ref'] = 'foregs.geochemical'\n",
    "\n",
    "temp['time'] = np.nan\n",
    "temp.loc[temp['time_real'].str.len() <= 4, 'time'] = temp.loc[temp['time_real'].str.len() <= 4, 'time_real'].astype(int)\n",
    "temp = temp.loc[temp['time']>=2000]\n",
    "temp = temp.drop(columns = ['time_real','gtn'])\n",
    "\n",
    "# possible filter\n",
    "na = temp['time'].isna().sum()\n",
    "print(f'{na} data with no time info')\n",
    "\n",
    "na = len(temp[temp['hzn_btm'].isna() | temp['hzn_top'].isna()])\n",
    "print(f'{na} data with no depth info')\n",
    "\n",
    "na = len(temp[temp['lat'].isna() | temp['lon'].isna()])\n",
    "print(f'{na} data with no coordinate info')\n",
    "\n",
    "print(f'{len(temp)} in total')\n",
    "\n",
    "temp.to_csv(f'{output_path}/foregs_harmonized_v1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96b819-b469-4158-8dd6-ede2755cbeda",
   "metadata": {},
   "source": [
    "### merge the dataset, filter based on time, coordinate, and depth info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "707a044e-ee44-4212-8e2e-91c641df49d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "germany:17189\n",
      "croatia:6271\n",
      "estonia:1343\n",
      "gemas:4132\n",
      "slovenia:6067\n",
      "swiss:36036\n",
      "netherlands:1070\n",
      "foregs:376\n",
      "basque:986\n",
      "spain.ParcelasCOS:1600\n",
      "spain.ParcelasINES:22160\n",
      "portugal:9934\n",
      "geocradle:1516\n",
      "lucas: 75460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1220/1414110373.py:12: DtypeWarning: Columns (7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lucas = pd.read_csv(f'{output_path}/lucas.full_harmonized_v1.csv')\n"
     ]
    }
   ],
   "source": [
    "# merge all the data\n",
    "names = ['germany','croatia','estonia','gemas','slovenia','swiss','netherlands','foregs','basque','spain.ParcelasCOS','spain.ParcelasINES','portugal','geocradle'] #'belgium','ireland','scotland', 'france'\n",
    "column_names = ['lat', 'lon', 'time', 'hzn_top', 'hzn_btm', 'ref', 'oc', 'ph_h2o', \n",
    "                'ph_cacl2', 'bulk_density', 'clay', 'silt', 'sand', 'caco3', 'N', 'K', 'P','CEC','EC']\n",
    "data = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for i in names:\n",
    "    temp = pd.read_csv(f'{output_path}/{i}_harmonized_v1.csv')\n",
    "    print(f'{i}:{len(temp)}')\n",
    "    data = pd.concat([data,temp])\n",
    "    \n",
    "lucas = pd.read_csv(f'{output_path}/lucas.full_harmonized_v1.csv')\n",
    "print(f'lucas: {len(lucas)}')\n",
    "data = pd.concat([data,lucas])\n",
    "data = data.drop(columns=['point_id','lc_survey','ph_kcl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da404223-c9af-4215-97c7-2554580336d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat: missing 0 data, 0%\n",
      "lon: missing 0 data, 0%\n",
      "time: missing 0 data, 0%\n",
      "hzn_top: missing 0 data, 0%\n",
      "hzn_btm: missing 0 data, 0%\n",
      "ref: missing 0 data, 0%\n",
      "oc: missing 17699 data, 13%\n",
      "ph_h2o: missing 53696 data, 39%\n",
      "ph_cacl2: missing 48158 data, 35%\n",
      "bulk_density: missing 79749 data, 58%\n",
      "clay: missing 55681 data, 40%\n",
      "silt: missing 81093 data, 59%\n",
      "sand: missing 81724 data, 59%\n",
      "caco3: missing 77564 data, 56%\n",
      "N: missing 53729 data, 39%\n",
      "K: missing 68296 data, 49%\n",
      "P: missing 74361 data, 54%\n",
      "CEC: missing 108493 data, 78%\n",
      "EC: missing 78914 data, 57%\n",
      "nuts0: missing 0 data, 0%\n",
      "cec: missing 138320 data, 100%\n",
      "sample_id: missing 136804 data, 99%\n"
     ]
    }
   ],
   "source": [
    "# only keep the data measured after 2000\n",
    "data = data.loc[data['time']>=2000]\n",
    "\n",
    "# drop rows without coordinates recorded\n",
    "data = data.loc[~data['lat'].isna()]\n",
    "\n",
    "# overview of the dataset\n",
    "for col in data.columns.values.tolist():\n",
    "    print(f'{col}: missing {data[col].isna().sum()} data, {round(data[col].isna().sum()*100/len(data))}%')\n",
    "    \n",
    "data.to_csv(f'{output_path}/soil.full_harmonized_v1.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7210da4d-8f08-4bae-9635-b5fb5ebf46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create gpkg\n",
    "# from shapely.geometry import Point\n",
    "# from geopandas import gpd\n",
    "\n",
    "# df = pd.read_csv('/mnt/primus/xuemeng_tmp_harbour/soc_eu/data/training_point_v2_full.csv', low_memory=False)\n",
    "# geometry = [Point(xy) for xy in zip(df['gps_long'], df['gps_lat'])]\n",
    "# gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# gdf_3035 = gdf.to_crs(\"EPSG:3035\")\n",
    "# gdf_3035['point_index'] = gdf_3035.index\n",
    "# gdf_3035.to_file(\"/mnt/primus/xuemeng_tmp_harbour/soc_eu/data/training_point_overlay_3035.gpkg\", driver=\"GPKG\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
